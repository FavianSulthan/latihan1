{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FavianSulthan/latihan1/blob/main/Latihan_1_KompStat_Kelompok_6_LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latihan 1 Komputasi Statistika\n",
        "Kelompok 6\n",
        "1. Adawia Ananda                 - 2106724883\n",
        "2. Aurelio Naufal Effendy        - 2106638526\n",
        "3. Favian Sulthan Wafi           - 2106706205\n",
        "4. Myra Azzahra Putri Syah Indra - 2106726844\n",
        "5. Rifqi Hafizuddin              - 2106638204\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rn5yyKhP0otm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study\n",
        "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.\n",
        "\n",
        "Gunakan 20 Newsgroup data berikut (Sumber Data). Bagaimana kita tahu topik apa yang dibicarakan dalam dokumen 20 Newsgroups tersebut? Mari lakukan implementasi topic modelling pada data tersebut."
      ],
      "metadata": {
        "id": "XOqNDSwn17Ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Modules"
      ],
      "metadata": {
        "id": "DzAU5J3j5CBd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1B3Gf33Zcqt",
        "outputId": "35f19b2c-4e16-4774-dc39-5a0aa627cd16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-29 08:20:48--  https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/taudataNlpTm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14001 (14K) [text/plain]\n",
            "Saving to: ‘taudataNlpTm.py’\n",
            "\n",
            "\rtaudataNlpTm.py       0%[                    ]       0  --.-KB/s               \rtaudataNlpTm.py     100%[===================>]  13.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-29 08:20:48 (88.2 MB/s) - ‘taudataNlpTm.py’ saved [14001/14001]\n",
            "\n",
            "--2022-09-29 08:20:48--  https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29041 (28K) [text/plain]\n",
            "Saving to: ‘data/slang.txt’\n",
            "\n",
            "slang.txt           100%[===================>]  28.36K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-09-29 08:20:48 (13.5 MB/s) - ‘data/slang.txt’ saved [29041/29041]\n",
            "\n",
            "--2022-09-29 08:20:48--  https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6446 (6.3K) [text/plain]\n",
            "Saving to: ‘data/stopwords_id.txt’\n",
            "\n",
            "stopwords_id.txt    100%[===================>]   6.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-29 08:20:49 (52.4 MB/s) - ‘data/stopwords_id.txt’ saved [6446/6446]\n",
            "\n",
            "--2022-09-29 08:20:49--  https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15140 (15K) [text/plain]\n",
            "Saving to: ‘data/stopwords_en.txt’\n",
            "\n",
            "stopwords_en.txt    100%[===================>]  14.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-29 08:20:49 (59.6 MB/s) - ‘data/stopwords_en.txt’ saved [15140/15140]\n",
            "\n",
            "--2022-09-29 08:20:49--  https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/kata_dasar.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 348507 (340K) [text/plain]\n",
            "Saving to: ‘data/kata_dasar.txt’\n",
            "\n",
            "kata_dasar.txt      100%[===================>] 340.34K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-09-29 08:20:49 (9.18 MB/s) - ‘data/kata_dasar.txt’ saved [348507/348507]\n",
            "\n",
            "--2022-09-29 08:20:49--  https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-ind-def.tab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 868322 (848K) [text/plain]\n",
            "Saving to: ‘data/wn-ind-def.tab’\n",
            "\n",
            "wn-ind-def.tab      100%[===================>] 847.97K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-09-29 08:20:49 (17.5 MB/s) - ‘data/wn-ind-def.tab’ saved [868322/868322]\n",
            "\n",
            "--2022-09-29 08:20:49--  https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-msa-all.tab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17003012 (16M) [text/plain]\n",
            "Saving to: ‘data/wn-msa-all.tab’\n",
            "\n",
            "wn-msa-all.tab      100%[===================>]  16.21M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-09-29 08:20:50 (139 MB/s) - ‘data/wn-msa-all.tab’ saved [17003012/17003012]\n",
            "\n",
            "--2022-09-29 08:20:50--  https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/ind_SA.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 918025 (897K) [text/plain]\n",
            "Saving to: ‘data/ind_SA.csv’\n",
            "\n",
            "ind_SA.csv          100%[===================>] 896.51K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-09-29 08:20:51 (18.6 MB/s) - ‘data/ind_SA.csv’ saved [918025/918025]\n",
            "\n",
            "--2022-09-29 08:20:51--  https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1722780 (1.6M) [application/octet-stream]\n",
            "Saving to: ‘data/all_indo_man_tag_corpus_model.crf.tagger’\n",
            "\n",
            "all_indo_man_tag_co 100%[===================>]   1.64M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-09-29 08:20:51 (30.1 MB/s) - ‘data/all_indo_man_tag_corpus_model.crf.tagger’ saved [1722780/1722780]\n",
            "\n",
            "--2022-09-29 08:20:51--  https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/dataset_tweet_sentiment_opini_film.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22487 (22K) [text/plain]\n",
            "Saving to: ‘data/dataset_tweet_sentiment_opini_film.csv’\n",
            "\n",
            "dataset_tweet_senti 100%[===================>]  21.96K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-09-29 08:20:51 (20.0 MB/s) - ‘data/dataset_tweet_sentiment_opini_film.csv’ saved [22487/22487]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.5-py3-none-any.whl (236 kB)\n",
            "\u001b[K     |████████████████████████████████| 236 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Collecting sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 44.3 MB/s \n",
            "\u001b[?25hCollecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 44.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.5)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.8.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.7.3)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Building wheels for collected packages: pyLDAvis, sklearn\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=cb5591844af77af82dab78aeef299d86dff39286127177744f84d533f4b97f69\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=bfb9eb00db3da163dd52a1a12e90e4a066ed59006f79b183a592f83084fca67a\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built pyLDAvis sklearn\n",
            "Installing collected packages: sklearn, funcy, unidecode, sastrawi, pyLDAvis\n",
            "Successfully installed funcy-1.17 pyLDAvis-3.3.1 sastrawi-1.0.1 sklearn-0.0 unidecode-1.3.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-crfsuite\n",
            "  Downloading python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Installing collected packages: python-crfsuite, gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0 python-crfsuite-0.9.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn-pycrfsuite\n",
            "  Downloading sklearn-pycrfsuite-0.4.0.tar.gz (24 kB)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-pycrfsuite) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-pycrfsuite) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-pycrfsuite) (0.8.10)\n",
            "Collecting python-crfsuite-extension\n",
            "  Downloading python-crfsuite-extension-0.9.7.tar.gz (485 kB)\n",
            "\u001b[K     |████████████████████████████████| 485 kB 6.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: sklearn-pycrfsuite, python-crfsuite-extension\n",
            "  Building wheel for sklearn-pycrfsuite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn-pycrfsuite: filename=sklearn_pycrfsuite-0.4.0-py2.py3-none-any.whl size=11002 sha256=0fdb9dde22735ee4510673d7915d8a190a1ff37c08ab2c713eb9ec745ba477c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/50/b1/295e502c6652f551dc99bfdb394a3fe2fcb47668e333391f38\n",
            "  Building wheel for python-crfsuite-extension (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-crfsuite-extension: filename=python_crfsuite_extension-0.9.7-cp37-cp37m-linux_x86_64.whl size=777992 sha256=6cbb3d62eb0448d0a6551feeb767200ec25ffb4fb97c0d5de5a1e59fc4e241c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/d7/bf/369f4d179407dbf3840132e4135fa61f0bf711f4496d0bf750\n",
            "Successfully built sklearn-pycrfsuite python-crfsuite-extension\n",
            "Installing collected packages: python-crfsuite-extension, sklearn-pycrfsuite\n",
            "Successfully installed python-crfsuite-extension-0.9.7 sklearn-pycrfsuite-0.4.0\n",
            "2022-09-29 08:21:57.167340: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xx-ent-wiki-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.4.0/xx_ent_wiki_sm-3.4.0-py3-none-any.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from xx-ent-wiki-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->xx-ent-wiki-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: xx-ent-wiki-sm\n",
            "Successfully installed xx-ent-wiki-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_ent_wiki_sm')\n",
            "2022-09-29 08:22:09.436388: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xx-sent-ud-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_sent_ud_sm-3.4.0/xx_sent_ud_sm-3.4.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from xx-sent-ud-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->xx-sent-ud-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: xx-sent-ud-sm\n",
            "Successfully installed xx-sent-ud-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_sent_ud_sm')\n",
            "2022-09-29 08:22:19.799656: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ],
      "source": [
        "# Sebelum load data, kita akan unduh beberapa resources untuk kita gunakan nantinya.\n",
        "# Cell hanya akan berjalan jika menggunakan Google Collab.\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "import nltk\n",
        "\n",
        "try:\n",
        "    import google.colab; IN_COLAB = True\n",
        "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/taudataNlpTm.py\n",
        "    !mkdir data\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/kata_dasar.txt\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-ind-def.tab\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-msa-all.tab\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/ind_SA.csv\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/dataset_tweet_sentiment_opini_film.csv\n",
        "    !pip install spacy unidecode textblob sastrawi pyLDAvis\n",
        "    !pip install --upgrade python-crfsuite gensim\n",
        "    !pip install sklearn-pycrfsuite\n",
        "    !python -m spacy download xx_ent_wiki_sm\n",
        "    !python -m spacy download xx_sent_ud_sm\n",
        "    !python -m spacy download en_core_web_sm\n",
        "\n",
        "    nltk.download('popular')\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Modules untuk Notebook ini\n",
        "import taudataNlpTm as tau, itertools, re, pickle, pyLDAvis, pyLDAvis.sklearn, spacy, urllib.request\n",
        "import numpy as np, matplotlib.pyplot as plt, pandas as pd, seaborn as sns \n",
        "from tqdm import tqdm\n",
        "from nltk.tag import CRFTagger\n",
        "from gensim.models import Phrases\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "sns.set(style=\"ticks\", color_codes=True)\n",
        "random_state = 99\n",
        "'Done'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "CAGI4DqBZu5h",
        "outputId": "11f236a4-c45b-4b60-af9f-31ee7f306013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n",
            "/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Mapping\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
            "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
            "  _deprecated()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Done'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "kRx087ix57dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data 20 Newsgroup\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "D = [doc for doc in documents]\n",
        "categories = dataset.target_names\n",
        "\n",
        "# Akan kita lihat berapa banyak dokumen yang ada di data ini\n",
        "print(\"Finished, {} documents loaded\".format(len(D)))"
      ],
      "metadata": {
        "id": "g9llnAYSZziB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Akan kita lihat kategori pada data\n",
        "num = []\n",
        "for i in range(1,len(categories)+1):\n",
        "  num.append(i)\n",
        "categories_df = pd.DataFrame(categories, index = num, columns =['Categories'])\n",
        "print(\"Terdapat {} kategori pada data\\n\".format(len(categories)))\n",
        "print(categories_df)\n"
      ],
      "metadata": {
        "id": "sFux4Ubw9UKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "jKJAzO2_7RwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk preprocessing, data akan diubah dari list ke bentuk pandas dataframe terlebih dahulu\n",
        "news_df = pd.DataFrame({'document':documents})\n",
        "\n",
        "# Mengganti tanda baca, angka, karakter spesial, dsb. dengan karakter kosong sehingga menyisakan huruf alfabet saja\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "\n",
        "# Menghilangkan kata-kata yang pendek(kurang dari 4 huruf) karena biasanya tidak mengandung informasi yang berguna\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "\n",
        "# Mengubah semua teks menjadi huruf kecil untuk menghilangkan case sensitivity\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "DTFVsBTZZ3Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stopwords (kami menggunakan Spacy)\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stop_words\n",
        "\n",
        "# Tokenisasi - mengubah kalimat menjadi kata-kata sehingga dapat diproses untuk menghilangkan stopwords\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "\n",
        "# Menghilangkan stopwords\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "\n",
        "# Detokenisasi - menggabungkan kembali kata-kata menjadi kalimat\n",
        "detokenized_doc = []\n",
        "for i in range(len(news_df)):\n",
        "    t = ' '.join(tokenized_doc[i])\n",
        "    detokenized_doc.append(t)\n",
        "\n",
        "news_df['clean_doc'] = detokenized_doc\n",
        "D = detokenized_doc"
      ],
      "metadata": {
        "id": "uiHITkL-k_A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding LDA Matrix Decomposition"
      ],
      "metadata": {
        "id": "Zdxq7VRsbnh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kita mulai dengan membuat VSM-nya (Vector Space Model)\n",
        "# Untuk LDA akan digunakan CountVectorizer dari sklearn untuk membuat Term-Document Matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "tf_vectorizer = CountVectorizer()\n",
        "\n",
        "data = D.copy()\n",
        "tf = tf_vectorizer.fit_transform(data)\n",
        "tf_terms = tf_vectorizer.get_feature_names()\n",
        "\n",
        "# Akan dilihat berapa banyak data pada dokumen\n",
        "tf.shape"
      ],
      "metadata": {
        "id": "lgAZhMacZ3V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selanjutnya akan dibentuk model LDA-nya\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "n_topics = 20 # Jumlah topik yang ingin ditelusuri. Kita ambil 20 topik karena terdapat 20 kategori pada dataset\n",
        "lda = LDA(n_components=n_topics, learning_method='batch', random_state=0).fit(tf)   \n",
        "vsm_topics = lda.transform(tf)\n",
        "vsm_topics.shape # Terlihat bahwa 64834 data pada dokumen terkompres menjadi topiknya sehingga hanya tersisa 20 data.\n",
        "                 # Hasil kompresi ini sangat baik karena sudah memuat struktur informasi pengelompokan kata pada data.\n",
        "                 # Proses ini juga memudahkan dan mempercepat komputasi"
      ],
      "metadata": {
        "id": "RNPTrF7NZ3fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modelling"
      ],
      "metadata": {
        "id": "MMGtYuD2yBng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mari kita coba maknai masing-masing topik ini\n",
        "Top_Words = 25 # Jumlah kata-kata yang paling sering muncul pada masing-masing topik\n",
        "print('Printing top {0} Topics, with top {1} Words:'.format(n_topics, Top_Words))\n",
        "tau.print_Topics(lda, tf_terms, n_topics, Top_Words)"
      ],
      "metadata": {
        "id": "tdNKED7san_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selanjutnya akan kita plot agar lebih jelas\n",
        "# Hasil plot dapat dilihat melalui Google Collab karena plot tidak muncul di Github\n",
        "import pyLDAvis==2.1.2, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
        "\n",
        "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)"
      ],
      "metadata": {
        "id": "RrIx7ZmPaoC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari visualisasi tersebut, dapat disimpulkan bahwa:\n",
        "*   Kata-kata yang paling menonjol dalam topik 1 antara lain: people, government, \n",
        "president, years, gun, money, dan killed. Dari kata-kata tersebut, dapat diprediksi bahwa topik 1 membicarakan tentang kebijakan pemerintahan.\n",
        "\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 2 antara lain: system, chip, data, encryption, information, computer, privacy, internet, public, dan security. Dari kata-kata tersebut, dapat diprediksi bahwa topik 2 membicarakan tentang keamanan cyber.\n",
        "\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 3 antara lain: people, jesus, believe, bible, christ, christian, faith, religion, dan hell. Dari kata-kata tersebut, dapat diprediksi bahwa topik 3 membicarakan tentang agama Kristen.\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 4 antara lain: file, windows, program, server, widget, application, system, dan information. Dari kata-kata tersebut, dapat diprediksi bahwa topik 4 membicarakan tentang sistem komputer.\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 5 antara lain: israel, government, jews, arab, public, political, dan security. Dari kata-kata tersebut, dapat diprediksi bahwa topik 5 membicarakan tentang kondisi politik Israel.\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 6 antara lain: card, windows, problem, system, software, memory, monitor, dan graphics. Dari kata-kata tersebut, dapat diprediksi bahwa topik 6 membicarakan tentang kartu grafik komputer.\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 7 antara lain:good, like, year, bike, player, average, game, dan team. Dari kata-kata tersebut, dapat diprediksi bahwa topik 7 membicarakan tentang permainan olahraga.\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 8 antara lain: people, church, science, christian, believe, catholic, dan mary. Dari kata-kata tersebut, dapat diprediksi bahwa topik 8 membicarakan tentang agama Katolik.\n",
        "\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 9 antara lain: team, game, hockey, season, players, league,dan rules. Dari kata-kata tersebut, dapat diprediksi bahwa topik 9 membicarakan tentang permainan hockey.\n",
        "\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 10 antara lain: armenian, turkish, genocide, jews, greek, muslim, islam, history, russian, soviet, ottoman, nazi, dan government. Dari kata-kata tersebut, dapat diprediksi bahwa topik 10 membicarakan tentang sejarah politik di Timur Tengah.\n",
        "\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 11 antara lain: time, engine, cars, miles, like, speed, dan scsi. Dari kata-kata tersebut, dapat diprediksi bahwa topik 11 membicarakan tentang mesin otomotif.\n",
        "\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 12 antara lain: space, nasa, launch, data, earth, satellite, shuttle, dan orbit engine. Dari kata-kata tersebut, dapat diprediksi bahwa topik 12 membicarakan tentang peluncuran ke luar angkasa.\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 13 antara lain: send, health, graphics, number, rate, mail, list, request, available, rates dan data. Dari kata-kata tersebut, dapat diprediksi bahwa topik 13 membicarakan tentang medis.\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 14 antara lain: university, april, good, center, york, national, washington, lost, internet, cancer, east, dan research. Dari kata-kata tersebut, dapat diprediksi bahwa topik 14 membicarakan tentang terdapat tempat penelitian kanker di Washington.\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 15 antara lain: patients, disease, nrhj, runs, game, wwiz, evidence, dan gizw. Dari kata-kata tersebut, dapat diprediksi bahwa topik 15 membicarakan tentang game medis.\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 16 antara lain: water, food, candida, cause, blood, father, doctor, body, dan medical. Dari kata-kata tersebut, dapat diprediksi bahwa topik 16 membicarakan tentang donasi untuk keperluan medis.\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 17 antara lain: period, play, power, flyers, second, scorer, shots, dan goal. Dari kata-kata tersebut, dapat diprediksi bahwa topik 17 membicarakan tentang liga hoki nasional Amerika Serikat.\n",
        "\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 18 antara lain: drive, disk, drives, hard, scsi, controller, floppy, bios, slave, dan system. Dari kata-kata tersebut, dapat diprediksi bahwa topik 18 membicarakan tentang elektronik.\n",
        "\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 19 antara lain: know, problem, work, xfree, like, problems, dan senior. Dari kata-kata tersebut, dapat diprediksi bahwa topik 19 membicarakan tentang permasalahan di kantor.\n",
        "\n",
        "\n",
        "*   Kata-kata yang paling menonjol dalam topik 20 antara lain: like, echo, think, mahan, daughter, maria, corn, bayonet, dan bullets. Dari kata-kata tersebut, dapat diprediksi bahwa topik 20 membicarakan tentang Mahan, seorang perwira angkatan laut amerika serikat.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U-IKBf3En7w0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referensi: \n",
        "*   https://github.com/gnespatel1618/TopicModeling/blob/master/Topic%20Modeling.ipynb\n",
        "*   https://taudata.blogspot.com/2022/05/nlptm-07.html\n",
        "\n"
      ],
      "metadata": {
        "id": "N_BjU3az5Nid"
      }
    }
  ]
}